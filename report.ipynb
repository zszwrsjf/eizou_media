{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 映像メディア課題\n",
    "今回は「Context Based Emotion Recognition using　EMOTIC Dataset」というTPAMI論文の自己風再現です。<br>\n",
    "https://github.com/Tandon-A/emotic での実現を参考しました。<br>\n",
    "ファイルの位置が人それぞれだから、最初にファイル位置を設定します。<br>\n",
    "別研究時間が必要のために、すべての例外処理がありません。すべての設定が正しいと仮定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "home_dir = os.environ['HOME']\n",
    "annotations_file = os.path.join(home_dir, 'data', 'Annotations', 'Annotations.mat')\n",
    "emotic_dir = os.path.join(home_dir, 'data', 'emotic')\n",
    "preprocessed_data_save_dir = os.path.join(home_dir, 'data', 'emotic_preprocessed')\n",
    "model_dir = os.path.join(home_dir, 'nothing', 'emotic', 'models')\n",
    "pretrained_model_dir = os.path.join(home_dir, 'pretrained_models')\n",
    "\n",
    "if not os.path.exists(preprocessed_data_save_dir):\n",
    "    os.makedirs(preprocessed_data_save_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from cmath import nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ前処理\n",
    "参考コードからAnnotations.matのデータ構造処理方法が早く分かるので、そちらからも参考させていただきました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations:dict = scio.loadmat(annotations_file)\n",
    "\n",
    "# 論文から見ると画像（Image）、人顔と体（Body）、感情ラベル（Emotion Categories）、\n",
    "# VADモデル点数（Continous Dimensions）を使って、画像感情分析をします。前の二つが入力、以外は出力です。\n",
    "# matをnpy(fastest)に変形します。そこでtrain, valid, testが分けたので、そのまま使います。\n",
    "\n",
    "emo_cats = ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion', 'Confidence', 'Disapproval', 'Disconnection',\n",
    "       'Disquietment', 'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem', 'Excitement', 'Fatigue', 'Fear',\n",
    "       'Happiness', 'Pain', 'Peace', 'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise', 'Sympathy', 'Yearning']\n",
    "cat2ind = {}\n",
    "ind2cat = {}\n",
    "for idx, emotion in enumerate(emo_cats):\n",
    "    cat2ind[emotion] = idx\n",
    "    ind2cat[idx] = emotion\n",
    "\n",
    "def emo_cat_one_hot(emo_cat):\n",
    "    one_hot_cat = np.zeros(26)\n",
    "    for em in emo_cat:\n",
    "        one_hot_cat[cat2ind[em]] = 1\n",
    "    return one_hot_cat\n",
    "\n",
    "data_types = ['train', 'val', 'test']\n",
    "for data_type in data_types:\n",
    "    cur_annotations = annotations[data_type]\n",
    "    image_arr, body_arr, emo_cat_arr, cont_dim_arr = [], [], [], []\n",
    "    for i, cur_annotation in enumerate(cur_annotations[0]):\n",
    "        print(data_type, ': {:.0%}'.format(i/len(cur_annotations[0])), end='\\r')\n",
    "        people_num = len(cur_annotation[4][0])\n",
    "        image_size = np.array(cur_annotation[2]).flatten().tolist()[0]\n",
    "        row = np.array(image_size[0]).flatten().tolist()[0]\n",
    "        col = np.array(image_size[1]).flatten().tolist()[0]\n",
    "        for person in range(people_num):\n",
    "            next_iter = False\n",
    "            person_details = cur_annotation[4][0][person]\n",
    "            emo_cat, cont_dim = [], []\n",
    "            if data_type == 'train':\n",
    "                emo_cat = np.array(person_details[1]).flatten().tolist()\n",
    "                emo_cat = np.array(emo_cat).flatten().tolist()\n",
    "                emo_cat = [np.array(c).flatten().tolist()[0] for c in emo_cat]\n",
    "                \n",
    "\n",
    "                cont_dim = np.array(person_details[2]).flatten().tolist()[0]\n",
    "                cont_dim = [np.array(c).flatten().tolist()[0] for c in cont_dim]\n",
    "                for c in cont_dim:\n",
    "                    if np.isnan(c):\n",
    "                        next_iter = True\n",
    "                        break\n",
    "                if next_iter: continue\n",
    "                \n",
    "            else:\n",
    "                if len(person_details[1][0]) != 0:\n",
    "                    emo_cat = [np.array(c).flatten().tolist()[0] for c in person_details[2][0]]\n",
    "\n",
    "                if len(person_details[3][0]) != 0:\n",
    "                    cont_dim = [np.array(c).flatten().tolist()[0] for c in person_details[4][0]]\n",
    "                    cont_dim = [np.array(c).flatten().tolist()[0] for c in cont_dim[0]]\n",
    "            \n",
    "            emo_cat = emo_cat_one_hot(emo_cat)\n",
    "            emo_cat_arr.append(emo_cat)\n",
    "            cont_dim_arr.append(np.array(cont_dim))\n",
    "\n",
    "            image_file = os.path.join(emotic_dir,cur_annotation[1][0], cur_annotation[0][0])\n",
    "            image = cv2.cvtColor(cv2.imread(image_file),cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            body_box = np.array(person_details[0]).flatten().tolist()\n",
    "            x1, y1, x2, y2 = map(int, body_box)\n",
    "            def relu(x):\n",
    "                return max(x, 0)\n",
    "            body = image[relu(y1):relu(y2), relu(x1):relu(x2)].copy()\n",
    "            body = cv2.resize(body, (128, 128))\n",
    "            body_arr.append(body)\n",
    "\n",
    "            image = cv2.resize(image, (224,224))\n",
    "            image_arr.append(image)\n",
    "\n",
    "    image_arr, body_arr, emo_cat_arr, cont_dim_arr\n",
    "    np.save(os.path.join(preprocessed_data_save_dir,'%s_context_arr.npy' %(data_type)), np.array(image_arr))\n",
    "    np.save(os.path.join(preprocessed_data_save_dir,'%s_body_arr.npy' %(data_type)), np.array(body_arr))\n",
    "    np.save(os.path.join(preprocessed_data_save_dir,'%s_cat_arr.npy' %(data_type)), np.array(emo_cat_arr))\n",
    "    np.save(os.path.join(preprocessed_data_save_dir,'%s_cont_arr.npy' %(data_type)), np.array(cont_dim_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.models import shufflenet_v2_x2_0, ShuffleNet_V2_X2_0_Weights, resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader を作ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_context = np.load(os.path.join(preprocessed_data_save_dir, 'train_context_arr.npy'))\n",
    "train_body = np.load(os.path.join(preprocessed_data_save_dir, 'train_body_arr.npy'))\n",
    "train_cat = np.load(os.path.join(preprocessed_data_save_dir, 'train_cat_arr.npy'))\n",
    "train_cont = np.load(os.path.join(preprocessed_data_save_dir, 'train_cont_arr.npy'))\n",
    "\n",
    "val_context = np.load(os.path.join(preprocessed_data_save_dir, 'val_context_arr.npy'))\n",
    "val_body = np.load(os.path.join(preprocessed_data_save_dir, 'val_body_arr.npy'))\n",
    "val_cat = np.load(os.path.join(preprocessed_data_save_dir, 'val_cat_arr.npy'))\n",
    "val_cont = np.load(os.path.join(preprocessed_data_save_dir, 'val_cont_arr.npy'))\n",
    "\n",
    "test_context = np.load(os.path.join(preprocessed_data_save_dir, 'test_context_arr.npy'))\n",
    "test_body = np.load(os.path.join(preprocessed_data_save_dir, 'test_body_arr.npy'))\n",
    "test_cat = np.load(os.path.join(preprocessed_data_save_dir, 'test_cat_arr.npy'))\n",
    "test_cont = np.load(os.path.join(preprocessed_data_save_dir, 'test_cont_arr.npy'))\n",
    "\n",
    "context_mean = [0.4690646, 0.4407227, 0.40508908]\n",
    "context_std = [0.2514227, 0.24312855, 0.24266963]\n",
    "body_mean = [0.43832874, 0.3964344, 0.3706214]\n",
    "body_std = [0.24784276, 0.23621225, 0.2323653]\n",
    "context_norm = [context_mean, context_std]\n",
    "body_norm = [body_mean, body_std]\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ColorJitter(\n",
    "                                          brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                                      transforms.ToTensor()])\n",
    "test_transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                     transforms.ToTensor()])\n",
    "\n",
    "class Emotic_PreDataset(Dataset):\n",
    "    def __init__(self, x_image, x_body, y_cat, y_cont, transform, context_norm, body_norm):\n",
    "        super(Emotic_PreDataset, self).__init__()\n",
    "        self.x_image = x_image\n",
    "        self.x_body = x_body\n",
    "        self.y_cat = y_cat\n",
    "        self.y_cont = y_cont\n",
    "        self.transform = transform\n",
    "        self.context_norm = transforms.Normalize(\n",
    "            context_norm[0], context_norm[1])\n",
    "        self.body_norm = transforms.Normalize(body_norm[0], body_norm[1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_cat)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_context = self.x_image[index]\n",
    "        image_body = self.x_body[index]\n",
    "        cat_label = self.y_cat[index]\n",
    "        cont_label = self.y_cont[index]\n",
    "        return self.context_norm(self.transform(image_context)), self.body_norm(self.transform(image_body)), torch.tensor(cat_label, dtype=torch.float32), torch.tensor(cont_label, dtype=torch.float32)/10.0\n",
    "\n",
    "train_dataset = Emotic_PreDataset(train_context, train_body, train_cat, train_cont,\n",
    "                                  train_transform, context_norm, body_norm)\n",
    "val_dataset = Emotic_PreDataset(val_context, val_body, val_cat, val_cont,\n",
    "                                test_transform, context_norm, body_norm)\n",
    "test_dataset = Emotic_PreDataset(test_context, test_body, test_cat, test_cont,\n",
    "                                 test_transform, context_norm, body_norm)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size,\n",
    "                          shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model構築をします。今回は論文で一番性能良いのBImodelを構築します。論文のモデル構造の図は16個convlutionを使ったけど、実際にいろんなpretrained model構造で試したので実験したので、今回はパラメータ数が少ないのShuffleNetを使いたいと思いますが、実験したところ、overfittingの問題が大変ですので、resnet16を使います。その上に、性能が一番いいBIモデルを構築します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = ShuffleNet_V2_X2_0_Weights.DEFAULT\n",
    "# model_body, model_image = shufflenet_v2_x2_0(weights=weights), shufflenet_v2_x2_0(weights=weights)\n",
    "\n",
    "class Fusion(nn.Module):\n",
    "    def __init__(self, nbf, nif):\n",
    "        super(Fusion, self).__init__()\n",
    "        self.nbf, self.nif = nbf, nif\n",
    "        self.fc = nn.Linear(nbf + nif, 256)\n",
    "        self.bn = nn.BatchNorm1d(256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dp = nn.Dropout(0.5)\n",
    "        self.fc_emo_cat = nn.Linear(256, 26)\n",
    "        self.fc_cont_dim = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, x_image, x_body):\n",
    "        x_image, x_body = x_image.view(-1, self.nif), x_body.view(-1, self.nbf)\n",
    "        fuse_features = torch.cat((x_image, x_body), 1)\n",
    "        fuse_out = self.fc(fuse_features)\n",
    "        fuse_out = self.bn(fuse_out)\n",
    "        fuse_out = self.relu(fuse_out)\n",
    "        fuse_out = self.dp(fuse_out)\n",
    "        emo_cat_out = self.fc_emo_cat(fuse_out)\n",
    "        cont_dim_out = self.fc_cont_dim(fuse_out)\n",
    "        return emo_cat_out, cont_dim_out\n",
    "\n",
    "model_image = resnet18(num_classes=365)\n",
    "context_state_dict = torch.load(os.path.join(\n",
    "    pretrained_model_dir, 'resnet18_state_dict.pth'))\n",
    "model_image.load_state_dict(context_state_dict)\n",
    "\n",
    "model_body = resnet18(weights=ResNet18_Weights)\n",
    "\n",
    "model_fusion = Fusion(list(model_image.children())[-1].in_features, list(model_body.children())[-1].in_features)\n",
    "model_image = nn.Sequential(*(list(model_image.children())[:-1]))\n",
    "model_body = nn.Sequential(*(list(model_body.children())[:-1]))\n",
    "\n",
    "# 低層の二つmodelを固定します\n",
    "for param in model_image.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_body.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_fusion.parameters():\n",
    "    param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training methodを設定します（GPU並行、最適化アルゴリズム、損失関数（SL1のほうが性能が一番です））"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model_image, model_body, model_fusion = nn.DataParallel(model_image), nn.DataParallel(model_body), nn.DataParallel(model_fusion)\n",
    "model_image.to(device)\n",
    "model_body.to(device)\n",
    "model_fusion.to(device)\n",
    "\n",
    "opt = optim.Adam((list(model_fusion.parameters()) + list(model_image.parameters()) +\n",
    "                  list(model_body.parameters())), lr=0.001, weight_decay=5e-4)\n",
    "scheduler = StepLR(opt, step_size=7, gamma=0.1)\n",
    "\n",
    "class Emo_cat_Loss(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Emo_cat_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        self.weights = self.prepare_dynamic_weights(target)\n",
    "        self.weights = self.weights.to(device)\n",
    "        loss = (((pred - target)**2) * self.weights)\n",
    "        return loss.sum()\n",
    "\n",
    "    def prepare_dynamic_weights(self, target):\n",
    "        target_stats = torch.sum(target, dim=0).float().unsqueeze(dim=0).cpu()\n",
    "        weights = torch.zeros((1, 26))\n",
    "        weights[target_stats != 0] = 1.0 / torch.log(target_stats[target_stats != 0].data + 1.2)\n",
    "        weights[target_stats == 0] = 0.0001\n",
    "        return weights\n",
    "\n",
    "class ContinuousLoss_SL1(nn.Module):\n",
    "    def __init__(self, margin=1):\n",
    "        super(ContinuousLoss_SL1, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        labs = torch.abs(pred - target)\n",
    "        loss = 0.5 * (labs ** 2)\n",
    "        loss[(labs > self.margin)] = labs[(labs > self.margin)] - 0.5\n",
    "        return loss.sum()\n",
    "cat_loss_func = Emo_cat_Loss()\n",
    "cont_loss_func = ContinuousLoss_SL1()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "\n",
    "    min_loss = np.inf\n",
    "\n",
    "    train_loss = list()\n",
    "    val_loss = list()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        model_fusion.train()\n",
    "        model_image.train()\n",
    "        model_body.train()\n",
    "        i = 1\n",
    "        for images_context, images_body, labels_cat, labels_cont in iter(train_loader):\n",
    "            print('train:{:.0%}'.format(i/len(train_loader)), end='\\r'); i += 1\n",
    "            images_context = images_context.to(device)\n",
    "            images_body = images_body.to(device)\n",
    "            labels_cat = labels_cat.to(device)\n",
    "            labels_cont = labels_cont.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            pred_image = model_image(images_context)\n",
    "            pred_body = model_body(images_body)\n",
    "            pred_cat, pred_cont = model_fusion(pred_image, pred_body)\n",
    "            cat_loss_batch = cat_loss_func(pred_cat, labels_cat)\n",
    "            cont_loss_batch = cont_loss_func(pred_cont * 10, labels_cont * 10)\n",
    "            loss = (0.5 * cat_loss_batch) + (0.5 * cont_loss_batch)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        print('epoch = %d training loss = %.4f' % (e, running_loss))\n",
    "        train_loss.append(running_loss)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        model_fusion.eval()\n",
    "        model_image.eval()\n",
    "        model_body.eval()\n",
    "\n",
    "        i = 1\n",
    "        with torch.no_grad():\n",
    "            for images_context, images_body, labels_cat, labels_cont in iter(val_loader):\n",
    "                print('valid:{:.0%}'.format(i/len(train_loader)), end='\\r'); i += 1\n",
    "                images_context = images_context.to(device)\n",
    "                images_body = images_body.to(device)\n",
    "                labels_cat = labels_cat.to(device)\n",
    "                labels_cont = labels_cont.to(device)\n",
    "\n",
    "                pred_context = model_image(images_context)\n",
    "                pred_body = model_body(images_body)\n",
    "\n",
    "                pred_cat, pred_cont = model_fusion(pred_context, pred_body)\n",
    "                cat_loss_batch = cat_loss_func(pred_cat, labels_cat)\n",
    "                cont_loss_batch = cont_loss_func(pred_cont * 10, labels_cont * 10)\n",
    "                loss = (0.5 * cat_loss_batch) + (0.5 * cont_loss_batch)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            if e % 1 == 0:\n",
    "                print('epoch = %d validation loss = %.4f' % (e, running_loss))\n",
    "        val_loss.append(running_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss[-1] < min_loss:\n",
    "            min_loss = val_loss[-1]\n",
    "            model_fusion.to(device)\n",
    "            model_image.to(device)\n",
    "            model_body.to(device)\n",
    "            torch.save(model_fusion, os.path.join(model_dir, 'model_emotic1.pth'))\n",
    "            torch.save(model_image, os.path.join(model_dir, 'model_context1.pth'))\n",
    "            torch.save(model_body, os.path.join(model_dir, 'model_body1.pth'))\n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 6))\n",
    "    f.suptitle('emotic')\n",
    "    ax1.plot(range(0, len(train_loss)), train_loss, color='Blue')\n",
    "    ax2.plot(range(0, len(val_loss)), val_loss, color='Red')\n",
    "    ax1.legend(['train'])\n",
    "    ax2.legend(['valid'])\n",
    "    plt.savefig('train_val_loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShuffleNetのoverfitting問題が大変ですので、やはり参考資料のpretrained_modelのResNet18を使う方がいいだと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_scikit_ap(cat_preds, cat_labels):\n",
    "  ap = np.zeros(26, dtype=np.float32)\n",
    "  for i in range(26):\n",
    "    ap[i] = average_precision_score(cat_labels[i, :], cat_preds[i, :])\n",
    "  print ('ap', ap, ap.shape, ap.mean())\n",
    "  return ap.mean()\n",
    "\n",
    "\n",
    "def test_emotic_vad(cont_preds, cont_labels):\n",
    "  vad = np.zeros(3, dtype=np.float32)\n",
    "  for i in range(3):\n",
    "    vad[i] = np.mean(np.abs(cont_preds[i, :] - cont_labels[i, :]))\n",
    "  print ('vad', vad, vad.shape, vad.mean())\n",
    "  return vad.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_scikit_ap(cat_preds, cat_labels):\n",
    "  ap = np.zeros(26, dtype=np.float32)\n",
    "  for i in range(26):\n",
    "    ap[i] = average_precision_score(cat_labels[i, :], cat_preds[i, :])\n",
    "  print ('ap', ap, ap.shape, ap.mean())\n",
    "  return ap.mean()\n",
    "\n",
    "\n",
    "def test_emotic_vad(cont_preds, cont_labels):\n",
    "  vad = np.zeros(3, dtype=np.float32)\n",
    "  for i in range(3):\n",
    "    vad[i] = np.mean(np.abs(cont_preds[i, :] - cont_labels[i, :]))\n",
    "  print ('vad', vad, vad.shape, vad.mean())\n",
    "  return vad.mean()\n",
    "\n",
    "def test(model_body, model_image, model_fusion):\n",
    "    cat_preds = np.zeros((7280, 26))\n",
    "    cat_labels = np.zeros((7280, 26))\n",
    "    cont_preds = np.zeros((7280, 3))\n",
    "    cont_labels = np.zeros((7280, 3))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_image.to(device)\n",
    "        model_body.to(device)\n",
    "        model_fusion.to(device)\n",
    "        model_image.eval()\n",
    "        model_body.eval()\n",
    "        model_fusion.eval()\n",
    "        indx = 0\n",
    "        i = 1\n",
    "        for images_context, images_body, labels_cat, labels_cont in iter(test_loader):\n",
    "            print(\"test:{:.0%}\".format(i/len(test_loader)), end='\\r'); i += 1\n",
    "            images_context = images_context.to(device)\n",
    "            images_body = images_body.to(device)\n",
    "\n",
    "            pred_context = model_image(images_context)\n",
    "            pred_body = model_body(images_body)\n",
    "            pred_cat, pred_cont = model_fusion(pred_context, pred_body)\n",
    "\n",
    "            cat_preds[ indx : (indx + pred_cat.shape[0]), :] = pred_cat.to(\"cpu\").data.numpy()\n",
    "            cat_labels[ indx : (indx + labels_cat.shape[0]), :] = labels_cat.to(\"cpu\").data.numpy()\n",
    "            cont_preds[ indx : (indx + pred_cont.shape[0]), :] = pred_cont.to(\"cpu\").data.numpy() * 10\n",
    "            cont_labels[ indx : (indx + labels_cont.shape[0]), :] = labels_cont.to(\"cpu\").data.numpy() * 10\n",
    "            indx = indx + pred_cat.shape[0]\n",
    "\n",
    "    cat_preds = cat_preds.transpose()\n",
    "    cat_labels = cat_labels.transpose()\n",
    "    cont_preds = cont_preds.transpose()\n",
    "    cont_labels = cont_labels.transpose()\n",
    "    ap_mean = test_scikit_ap(cat_preds, cat_labels)\n",
    "    vad_mean = test_emotic_vad(cont_preds, cont_labels)\n",
    "    print (ap_mean, vad_mean)\n",
    "    return ap_mean, vad_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_body = torch.load(os.path.join(model_dir, 'model_context1.pth'))\n",
    "model_image = torch.load(os.path.join(model_dir, 'model_body1.pth'))\n",
    "model_emotic1 = torch.load(os.path.join(model_dir, 'model_emotic1.pth'))\n",
    "test(model_body, model_image, model_fusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "731b0c542c61d1f0ca5d836437176e955db409ef8752c57fa944d975751c0e62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
